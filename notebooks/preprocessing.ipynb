{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983eeebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.utils import Bunch\n",
    "import random\n",
    "\n",
    "# Choose a dataset: IMDB, SST-2, or AG News\n",
    "dataset_name = \"AG_News\"  # Change this to \"SST-2\" or \"AG News\" as needed\n",
    "\n",
    "if dataset_name:\n",
    "    dataset = fetch_openml(\"AG_News\", version=1)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported dataset. Choose IMDB, SST-2, or AG News.\")\n",
    "\n",
    "train_split = 0.8\n",
    "\n",
    "# Create a small dev subset for quick iteration\n",
    "def create_dev_subset(dataset: 'Bunch', subset_size=100):\n",
    "    rows_df = dataset.data\n",
    "\n",
    "    train_df = rows_df[:int(len(rows_df) * train_split)]\n",
    "    indices = random.sample(range(len(train_df)), subset_size)\n",
    "    dev_subset = train_df.iloc[indices]\n",
    "    return dev_subset\n",
    "\n",
    "dev_subset = create_dev_subset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01052402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Basic unicode cleanup\n",
    "    text = text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "    return text\n",
    "\n",
    "dev_subset.Description = dev_subset.Description.apply(normalize_text)\n",
    "dev_subset.Title = dev_subset.Title.apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87850b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/manish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/manish/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK tokenizer resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tokens = nltk.word_tokenize(\"This is a sample sentence for tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14257224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34260324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize: compare whitespace split, NLTK tokenizers, and spaCy tokenization\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "dev_subset['Title_Tokens'] = dev_subset.Title.apply(tokenize_text)\n",
    "dev_subset['Description_Tokens'] = dev_subset.Description.apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# stemming (Porter) vs lemmatization (spaCy), stopword removal.\n",
    "\n",
    "# Initialize Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming and lemmatization, and remove stopwords\n",
    "def process_tokens(tokens):\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token.lower() not in STOP_WORDS]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Apply lemmatization using spaCy\n",
    "    lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "dev_subset['Title_Processed'] = dev_subset.Title_Tokens.apply(process_tokens)\n",
    "dev_subset['Description_Processed'] = dev_subset.Description_Tokens.apply(process_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c236869",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#    - Build `CountVectorizer` and `TfidfVectorizer` (unigram and unigram+bigram).\n",
    "#    - Use `max_features` to cap vocabulary size for faster PyTorch models (e.g., 10k–50k).\n",
    "#    - TF‑IDF output is typically a sparse matrix (CSR). For small dev experiments you can convert to dense with `.toarray()`; for larger data keep it sparse and sample mini-batches.\n",
    "\n",
    "# Cap vocabulary for faster experiments\n",
    "max_features = 10_000\n",
    "\n",
    "def build_vectorizers(text_series, prefix):\n",
    "    results = {}\n",
    "    for VecClass, kind in ((CountVectorizer, \"count\"), (TfidfVectorizer, \"tfidf\")):\n",
    "        for ngram_range in ((1, 1), (1, 2)):\n",
    "            key = f\"{prefix}_{kind}_{ngram_range[0]}_{ngram_range[1]}\"\n",
    "            vec = VecClass(ngram_range=ngram_range, max_features=max_features)\n",
    "            X = vec.fit_transform(text_series)\n",
    "            # For small dev experiments convert to dense for easier inspection\n",
    "            if len(text_series) <= 1000:\n",
    "                X_out = X.toarray()\n",
    "            else:\n",
    "                X_out = X\n",
    "            results[key] = {\"vectorizer\": vec, \"matrix\": X_out}\n",
    "    return results\n",
    "\n",
    "# Build vectorizers for Title and Description\n",
    "title_features = build_vectorizers(dev_subset.Title, \"title\")\n",
    "description_features = build_vectorizers(dev_subset.Description, \"description\")\n",
    "\n",
    "# Quick summary of resulting matrices / vocab sizes\n",
    "for d in (title_features, description_features):\n",
    "    for name, info in d.items():\n",
    "        mat = info[\"matrix\"]\n",
    "        vocab_size = len(info[\"vectorizer\"].vocabulary_)\n",
    "        print(f\"{name}: matrix shape = {mat.shape}, vocab_size = {vocab_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
